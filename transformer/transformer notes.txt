transformer notes:

for V, Q, K vectors(and associated matrices)
- one dim is gonna be the same as embedding length ofc
- output of the context-sensitive embedding should also be that width

since concatenation like addition, can definitely concatenate the 
positional embedding to the regular word embedding

rememb, the positional stuff goes before extracting the 
query, key, value vectors

need a mask to prevent looking beyond the word in question?????
the attention mask matrix is a simple mod to the regular one
where its just M added to the KQ^t
- essentially, a word can only pay attention to itself and the 
  stuff before it. no future words
  > so this would look like the top right *triangle* of the mask
    matrix as zeros (see np.triu_indices)
  > but because of the softmax at the end of the attention layer,  
    have to use to - infinity instead
  > again, the mask only necessary for a language model element
    with embeddngs (but even MT needs embedding!)

basis of multi-head attention is taking the same E' equation, doing 
it multiple times, and concatenating the outputs

at first we were going to have the transform matrices for K, Q, and V
be size e x e where e is the embedding size, but for multihead attn
this is gonna change

need output size of E' to be 1/x the usual size, where x is the
# of times we're applying E for multihead attn
- can just achieve this by changing the size of the transform vectors
  to 1/x
> they say 8 is the "magic number" of multi attn heads

weird tweak with the transformer in the paper is that the pre attention
inputs are ADDED to the output of attn...
- we're getting a sense of the difference between the og input and 
  the output now!

new full E' vector is now (E+PosEnc) + Attn(E+PosEnc)

need a "highway" to make the input directly feed into the output
- for some reason, doing the regular linear+RELU way doesn't work?
> the adding we do is that direct part

theres a feed forward after the attn with Add & Norm that also gets
done howevery many times (magic #8?)